# ðŸ Python Backend - Transcription & MCQ Generator (FastAPI + Whisper + Ollama)

This backend service handles:

- ðŸŽ™ï¸ Transcribing video/audio using OpenAI Whisper
- ðŸ§  Generating multiple-choice questions (MCQs) from transcript segments using a local LLM via Ollama
- ðŸ“¡ Exposing API endpoints for integration with frontend or other backend services (like Node.js)

---

## ðŸ“¦ Tech Stack

- **FastAPI** â€“ lightweight and fast web API framework
- **Whisper** â€“ speech-to-text transcription (supports local or cloud inference)
- **Ollama** â€“ local LLM inference (e.g., Mistral, LLaMA2)
- **Pydantic** â€“ data validation and serialization

---

## ðŸš€ Features

- ðŸ”¹ `/transcribe` â€” Transcribe a local video/audio file from a path or URL
- ðŸ”¹ `/generate-mcqs` â€” Generate MCQs from a transcript segment using a strict JSON format
- ðŸ”¹ Optimized for backend-to-backend communication with Node.js

---

## ðŸ§° Requirements

- Python 3.10+
- `ffmpeg` installed (used by Whisper)
- Ollama installed and running locally

---

## ðŸ“‚ Folder Structure

```
python-backend/
â”œâ”€â”€ main.py                  # FastAPI entrypoint
â”œâ”€â”€ transcription_service.py # Handles transcription using Whisper
â”œâ”€â”€ mcq_service.py           # MCQ generation using Ollama
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                # You are here
```

---

## âš™ï¸ Setup Instructions

### 1. âœ… Install Dependencies

First, create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

Then install dependencies:

```bash
pip install -r requirements.txt
```

> ðŸ’¡ Optional: Use `faster-whisper` instead of `openai-whisper` for performance.

### 3. ðŸ§  Install & Run Ollama

Install Ollama ([https://ollama.com](https://ollama.com)) and run a model (like `gemma:2b`):

Install Gemma:2b

```bash
ollama pull gemma:2b
```

Ollama should be running locally on port `11434`.

---

## ðŸš€ Run FastAPI Server

Start the FastAPI app:

```bash
uvicorn main:app --reload --port 8000
```

Your API will now be available at:

```
http://localhost:8000
```

---

## ðŸ”Œ API Endpoints

### ðŸ“ `POST /transcribe`

Transcribes a video or audio file.

**Request:**

```json
{
  "file_path": "/absolute/or/http/path/to/video.mp4"
}
```

**Response:**

```json
{
  "text": "Full transcript text here...",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 4.5,
      "text": "This is a segment..."
    },
    ...
  ]
}
```

---

### ðŸ“ `POST /generate-mcqs`

Generates MCQs based on a given transcript segment.

**Request:**

```json
{
  "segment_text": "This paragraph talks about the importance of X..."
}
```

**Response:**

```json
{
  "mcqs": [
    {
      "question": "What is the importance of X?",
      "options": ["A. Reason 1", "B. Reason 2", "C. Reason 3", "D. Reason 4"],
      "answer": "A. Reason 1"
    },
    ...
  ]
}
```

> âœ… Response format is strictly enforced via the LLM prompt.

---

## ðŸ§ª Testing

### Using `curl`:

```bash
curl -X POST http://localhost:8000/generate-mcqs \
  -H "Content-Type: application/json" \
  -d '{"segment_text": "Sample text for testing MCQ generation."}'
```

---

## ðŸ›  Prompt Used for MCQs

To maintain consistent formatting from Ollama:

```
Generate 3 multiple choice questions based on the following transcript segment:

[segment_text_here]

Return only JSON in this format:
[
  {
    "question": "...",
    "options": ["A. ...", "B. ...", "C. ...", "D. ..."],
    "answer": "A. ..."
  }
]
```

---

## ðŸ§© Improvements Roadmap

- [ ] Support chunked transcription streaming
- [ ] Use faster-whisper for efficiency
- [ ] Add support for different MCQ difficulty levels
- [ ] Support image/audio MCQ types in the future

---

## ðŸ§¼ Cleanup

To clean up the virtual environment:

```bash
deactivate
rm -rf venv
```

---
